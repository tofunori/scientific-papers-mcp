# ===================================================================
# Scientific Papers MCP Server - Environment Configuration
# ===================================================================
# Copy this file to .env and fill in your values

# ===================================================================
# PDF Extraction Settings
# ===================================================================
# PDF extraction method: "pymupdf" (default), "marker_api", "marker_local", or "llamaparse"
PDF_EXTRACTION_METHOD=pymupdf

# Marker API Configuration (https://www.datalab.to/)
# Sign up at https://www.datalab.to/ to get $5 free credits
MARKER_API_KEY=your_marker_api_key_here
MARKER_USE_LLM=true                  # Enable LLM for better quality (recommended)
MARKER_FORCE_OCR=false               # Force OCR even if text is embedded
MARKER_API_TIMEOUT=180               # Timeout in seconds (3 minutes)

# Marker Local Configuration (requires: pip install marker-pdf)
MARKER_LOCAL_BATCH_MULTIPLIER=1      # GPU batch multiplier (1-4, higher=more GPU usage)
MARKER_LOCAL_USE_LLM=false           # Use LLM with local Marker (requires API key)

# Marker Local LLM Settings
# Choose LLM service: gemini (default), vertex, claude, openai, ollama
MARKER_LOCAL_LLM_SERVICE=gemini

# Gemini Configuration (Default for Marker --use_llm)
# Get key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=                      # Gemini API key
MARKER_GEMINI_MODEL=gemini-2.0-flash # Model: gemini-2.0-flash, gemini-2.5-flash, etc.

# Google Vertex AI (Alternative to Gemini, more reliable for production)
MARKER_VERTEX_PROJECT_ID=            # GCP Project ID

# Claude Configuration (Alternative LLM)
# Get key from: https://console.anthropic.com/
CLAUDE_API_KEY=                      # Anthropic Claude API key
MARKER_CLAUDE_MODEL=claude-3-sonnet-20240229  # Model name

# OpenAI Configuration (Alternative LLM)
# Get key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=                      # OpenAI API key
MARKER_OPENAI_MODEL=gpt-4-turbo      # Model: gpt-4-turbo, gpt-4, gpt-3.5-turbo

# Ollama Configuration (Local LLM, completely free)
# Install: https://ollama.ai/
MARKER_OLLAMA_BASE_URL=http://localhost:11434  # Ollama server URL
MARKER_OLLAMA_MODEL=llama2           # Model: llama2, mistral, mixtral, etc.

# Fallback Settings
MARKER_FALLBACK_TO_PYMUPDF=true      # Fallback to PyMuPDF if Marker fails

# ===================================================================
# LlamaParse Configuration (GenAI-native PDF parsing)
# ===================================================================
# LlamaParse offers affordable, high-quality PDF parsing with natural language instructions
# Pricing: $0.003/page with 7,000 FREE pages per week
# API Documentation: https://docs.cloud.llamaindex.ai/
# Get API key from: https://cloud.llamaindex.ai/

# LlamaCloud API Key (used for both LlamaParse and LlamaExtract)
LLAMA_CLOUD_API_KEY=your_llamacloud_api_key_here

# LlamaParse Settings
LLAMAPARSE_RESULT_TYPE=markdown               # Output format: "markdown" or "text"
LLAMAPARSE_PARSING_INSTRUCTION=               # Custom parsing instructions (optional)
LLAMAPARSE_USE_VENDOR_MULTIMODAL=false        # Use vendor multimodal models (more expensive)
LLAMAPARSE_NUM_WORKERS=4                      # Number of parallel workers
LLAMAPARSE_MAX_TIMEOUT=2000                   # Maximum timeout in seconds
LLAMAPARSE_INVALIDATE_CACHE=false             # Force re-parsing (ignore cache)

# LlamaExtract Settings (Structured metadata extraction)
LLAMAEXTRACT_ENABLED=false                    # Enable structured metadata extraction
LLAMAEXTRACT_SCHEMA_NAME=scientific_paper     # Extraction schema to use

# ===================================================================
# Paths
# ===================================================================
DOCUMENTS_PATH=C:/Users/thier/Zotero/storage
CHROMA_PATH=D:/Claude Code/scientific-papers-mcp/data/chroma
INDEXING_STATE_PATH=D:/Claude Code/scientific-papers-mcp/data/indexing_state.json

# ===================================================================
# Chroma Cloud Configuration (Optional)
# ===================================================================
USE_CHROMA_CLOUD=false
CHROMA_API_KEY=your_chroma_cloud_api_key
CHROMA_TENANT=your_tenant_id
CHROMA_DATABASE=Scientific
SYNC_TO_CLOUD=true
DEFAULT_SEARCH_SOURCE=local

# ===================================================================
# Embedding Configuration
# ===================================================================
# Voyage AI (Recommended for scientific papers)
USE_VOYAGE_API=true
VOYAGE_API_KEY=your_voyage_api_key
VOYAGE_TEXT_MODEL=voyage-context-3
VOYAGE_MULTIMODAL_MODEL=voyage-multimodal-3

# Jina AI (Alternative)
USE_JINA_API=false
JINA_API_KEY=your_jina_api_key
JINA_MODEL=jina-embeddings-v4

# Local embedding (Fallback)
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_DIMENSIONS=1024

# ===================================================================
# Search & Reranking Configuration
# ===================================================================
# Hybrid search settings
DEFAULT_ALPHA=0.5                    # 0=keyword only, 1=semantic only
USE_RRF=true                         # Use Reciprocal Rank Fusion
RRF_K_PARAMETER=60
RRF_DENSE_WEIGHT=0.7
RRF_SPARSE_WEIGHT=0.3

# Cohere Reranking (Recommended)
USE_COHERE_RERANK=true
COHERE_API_KEY=your_cohere_api_key
COHERE_MODEL=rerank-v3.5
COHERE_TOP_K=10

# Local reranking (Fallback)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_TOP_K=50

# ===================================================================
# Chunking Configuration
# ===================================================================
CHUNKING_ENABLED=true
CHUNK_SIZE=1024                      # Tokens per chunk (optimal for scientific papers)
CHUNK_OVERLAP=100                    # Token overlap
CHUNK_ENCODING=cl100k_base           # GPT tokenizer
ENABLE_CONTEXTUAL_CHUNKING=true

# ===================================================================
# Indexing Configuration
# ===================================================================
AUTO_INDEX_ON_START=false
WATCH_DIRECTORY=true
ENABLE_INCREMENTAL_INDEXING=true
ENABLE_DEDUPLICATION=true
TRACK_FILE_MODIFICATIONS=true
BATCH_INDEXING_SIZE=50

# ===================================================================
# Collection & Search Settings
# ===================================================================
DEFAULT_COLLECTION_NAME=scientific_papers_voyage
DEFAULT_TOP_K=10

# ===================================================================
# Logging
# ===================================================================
LOG_LEVEL=INFO
