# Scientific Papers MCP Server

A powerful Model Context Protocol (MCP) server for intelligent semantic and full-text search across a collection of scientific papers on glaciology, climate, and environmental research.

## ğŸ¯ What This MCP Does

The **Scientific Papers MCP** enables Claude and other AI assistants to search through scientific research papers with both semantic understanding and precise keyword matching. It acts as a bridge between your AI and your document collection, handling:

- **Intelligent Document Indexing**: Automatically processes Markdown and PDF documents (including OCR for scanned papers)
- **Hybrid Search**: Combines semantic similarity (AI understands meaning) with keyword matching (precise text search)
- **Fast Vector Database**: Uses ChromaDB with vector embeddings for AI-powered search
- **Metadata Extraction**: Automatically extracts authors, publication year, datasets, instruments, and tags
- **Smart Chunking**: Breaks documents intelligently to preserve context

## ğŸš€ Key Features

| Feature | Description |
|---------|-------------|
| **Hybrid Search** | Combines semantic embeddings + BM25 keyword search for best results |
| **Multi-Format Support** | Handles Markdown, PDFs (text), and scanned PDFs (with OCR) |
| **Metadata Extraction** | Auto-detects year, authors, datasets, instruments via regex patterns |
| **Full-Text Search** | Supports regex, wildcards, AND/OR operators for precise queries |
| **Smart Chunking** | Respects document structure (sections, paragraphs) during indexing |
| **Fast Inference** | ~50-250ms search latency depending on method |
| **Multilingual Support** | Works with 100+ languages via multilingual-e5-large embeddings |

## ğŸ“š How It Works: Technical Overview

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Your Documents                            â”‚
â”‚              (Markdown, PDF, Scanned PDFs)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Document Processing       â”‚
        â”‚  â”œâ”€ PDF/Text Extraction      â”‚
        â”‚  â”œâ”€ OCR for Scanned PDFs     â”‚
        â”‚  â””â”€ Metadata Extraction      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Intelligent Chunking      â”‚
        â”‚  â”œâ”€ Respect Section Structureâ”‚
        â”‚  â”œâ”€ Preserve Context         â”‚
        â”‚  â””â”€ Optimize Token Count     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
        â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Embeddings    â”‚         â”‚   BM25 Tokens    â”‚
    â”‚ (multilingual-  â”‚         â”‚  (Keyword Index) â”‚
    â”‚  e5-large)      â”‚         â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    ChromaDB     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Vector Database â”‚
    â”‚  Vector Store   â”‚         â”‚  + BM25 Index    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Hybrid Search Engine    â”‚
    â”‚  â”œâ”€ Semantic Search       â”‚
    â”‚  â”œâ”€ Keyword Search        â”‚
    â”‚  â””â”€ Result Fusion (Alpha) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ranked Results      â”‚
    â”‚  (Top-K matches)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Hybrid Search Pipeline

**Why Hybrid?** One search method alone isn't enough:
- **Semantic search** understands meaning but can miss specific terms
- **Keyword search** finds exact terms but doesn't understand context

Our hybrid approach combines both:

```
Query: "glacier albedo feedback mechanisms"
       â”‚
       â”œâ”€â†’ [SEMANTIC SEARCH]
       â”‚   â€¢ Convert to embeddings (vector space)
       â”‚   â€¢ Find semantically similar documents
       â”‚   â€¢ Returns: {"doc1": 0.95, "doc2": 0.87, "doc3": 0.72}
       â”‚
       â””â”€â†’ [KEYWORD SEARCH (BM25)]
           â€¢ Search for exact terms
           â€¢ TF-IDF ranking
           â€¢ Returns: {"doc1": 0.89, "doc2": 0.65, "doc4": 0.58}

       â†“ [FUSION - Controlled by Alpha parameter]

       â”œâ”€ Alpha = 1.0  â†’ 100% semantic, 0% keyword
       â”œâ”€ Alpha = 0.5  â†’ 50% semantic, 50% keyword (RECOMMENDED)
       â””â”€ Alpha = 0.0  â†’ 0% semantic, 100% keyword

       â†“ [FINAL RANKING]

       Result: {"doc1": 0.92, "doc2": 0.76, "doc3": 0.65, "doc4": 0.29}
```

### The Embedding Model

The MCP uses **multilingual-e5-large** (560M parameters) for text embeddings:

- Converts text â†’ 1024-dimensional vectors
- Trained on 1 billion text pairs with contrastive learning
- Supports 100+ languages
- Fast inference (~10ms per document)
- State-of-the-art on MTEB benchmark for multilingual retrieval

**Alternative models** (if you want to upgrade):
- `Qwen/Qwen3-Embedding-8B` - Newer (v2025), even better, needs 16GB+ RAM
- `allenai/specter2` - Specialized for scientific papers, English only

## ğŸ”§ Installation & Setup

### Prerequisites

- Python 3.10+
- pip or uv package manager
- (Optional) Tesseract OCR for scanned PDFs

### Installation

1. **Clone and enter the project**
```bash
git clone https://github.com/tofunori/scientific-papers-mcp.git
cd scientific-papers-mcp
```

2. **Install dependencies**
```bash
pip install -e .
```

Or with uv (faster):
```bash
uv pip install -e .
```

3. **Configure paths** in `config.py` or `.env`
```python
DOCUMENTS_PATH = "path/to/your/papers"  # Markdown & PDFs
CHROMA_PATH = "path/to/chroma/db"
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
```

4. **(Optional) Install Tesseract OCR** for scanned PDFs

**Windows**: Download from https://github.com/UB-Mannheim/tesseract-ocr

**Linux (Debian/Ubuntu)**:
```bash
sudo apt-get install tesseract-ocr
```

**macOS**:
```bash
brew install tesseract
```

## ğŸ’¡ Quick Start

### Method 1: Using Claude Code

Add to your Claude Code config:

```json
{
  "mcpServers": {
    "scientific-papers": {
      "command": "python",
      "args": ["-m", "src.server"]
    }
  }
}
```

Then in Claude:
```
Search for articles about glacier albedo feedback
Find papers mentioning MODIS and MOD10A1
```

### Method 2: Python Script

```python
from src.indexing.chroma_client import initialize_chroma
from src.indexing.hybrid_search import HybridSearchEngine

# Initialize once
chroma_collection = initialize_chroma("./data/chroma")
search_engine = HybridSearchEngine(chroma_collection)

# Perform hybrid search
doc_ids, scores, documents, metadata = search_engine.hybrid_search(
    query="glacier albedo feedback",
    top_k=5,
    alpha=0.5  # 50% semantic, 50% keyword
)

# Process results
for doc_id, score, text, meta in zip(doc_ids, scores, documents, metadata):
    print(f"Match: {score:.2%}")
    print(f"Title: {meta.get('title', 'Unknown')}")
    print(f"Authors: {meta.get('authors', 'Unknown')}")
    print(f"Text: {text[:200]}...\n")
```

## ğŸ—ï¸ Architecture & Components

### Directory Structure

```
src/
â”œâ”€â”€ server.py                    # MCP server entry point (FastMCP)
â”œâ”€â”€ config.py                    # Configuration manager
â”‚
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ pdf_extractor.py        # PDF text & metadata extraction
â”‚   â”œâ”€â”€ metadata_extractor.py    # Regex-based metadata parsing
â”‚   â””â”€â”€ patterns.py              # Regex patterns for metadata
â”‚
â”œâ”€â”€ indexing/
â”‚   â”œâ”€â”€ chroma_client.py        # Vector DB initialization & queries
â”‚   â”œâ”€â”€ chunker.py              # Document chunking (respects structure)
â”‚   â””â”€â”€ hybrid_search.py        # Semantic + keyword search fusion
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ search_tools.py         # MCP tools for searching
â”‚   â””â”€â”€ metadata_tools.py       # MCP tools for metadata queries
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ logger.py               # Structured logging
    â””â”€â”€ file_watcher.py         # Auto-indexing on file changes
```

### Data Processing Pipeline

```
INDEXING (One-time, on startup)
â”œâ”€ Scan documents folder (Markdown + PDFs)
â”œâ”€ Extract text from each document
â”œâ”€ Extract metadata (authors, year, etc.)
â”œâ”€ Chunk respecting structure
â”œâ”€ Generate embeddings (sentence-transformers)
â”œâ”€ Tokenize for BM25
â””â”€ Store in ChromaDB + BM25 index

SEARCHING (Per query, real-time)
â”œâ”€ User sends query
â”œâ”€ Generate query embeddings
â”œâ”€ SEMANTIC: Find nearest vectors in ChromaDB
â”œâ”€ KEYWORD: BM25 score matching
â”œâ”€ Fusion: Combine scores using alpha
â”œâ”€ Rank and return top-K results
â””â”€ Return with metadata & relevance scores
```

## ğŸ” Search Features

### 1. Hybrid Search (Recommended)

Balances semantic understanding with keyword precision:

```python
results, scores = search_engine.hybrid_search(
    query="glacier albedo feedback mechanisms",
    top_k=5,
    alpha=0.5  # Adjust 0.0-1.0
)
```

**When to adjust alpha:**
- `alpha=1.0`: Very abstract queries ("climate change impacts")
- `alpha=0.5`: Balanced queries (recommended default)
- `alpha=0.0`: Very specific/technical queries ("MODIS MOD10A1")

### 2. Full-Text Search (Precise)

For exact text matching with regex support:

```python
# Simple contains
results = search_engine.search(
    query="satellite",
    where_document={"$contains": "MODIS"}
)

# Regex pattern
results = search_engine.search(
    query="sensor",
    where_document={"$regex": "MOD[0-9]{2}A[0-9]"}
)

# Boolean logic
results = search_engine.search(
    query="glacier",
    where_document={
        "$and": [
            {"$contains": "albedo"},
            {"$contains": "Alaska"}
        ]
    }
)
```

**Available operators:**
| Operator | Use Case |
|----------|----------|
| `$contains` | Substring search |
| `$regex` | Regular expressions |
| `$and` | All conditions must match |
| `$or` | Any condition can match |
| `$not_contains` | Exclude results |

## ğŸ“„ Supported Document Formats

### Markdown (.md)

Best for:
- Structured notes
- Research summaries
- Already-formatted content

Features:
- Hierarchical structure respected (headers)
- Metadata in frontmatter
- Clean chunking by sections

Example:
```markdown
# Paper Title
**Authors:** Smith et al.
**Year:** 2023

## Introduction
...

## Methods
...
```

### PDF - Text-based

For standard PDFs with extractable text:
- Native text extraction (fast)
- Metadata from PDF properties
- Automatic chunking by paragraphs

### PDF - Scanned (OCR)

For scanned documents/images:
- Optical Character Recognition (Tesseract)
- Slower (~100-500ms per page)
- Fallback metadata extraction via regex

```python
from src.extractors.pdf_extractor import extract_text_from_pdf

text, is_scanned = extract_text_from_pdf("scanned_paper.pdf")
# Returns: (text, True) if OCR was used
```

## âš™ï¸ Configuration Reference

### Environment Variables (config.py)

```python
# Document paths
DOCUMENTS_PATH = "D:/path/to/papers"          # Where to find files
CHROMA_PATH = "D:/path/to/chroma/db"         # Vector DB location

# Search settings
DEFAULT_TOP_K = 10                            # Results per query
DEFAULT_ALPHA = 0.5                           # Semantic vs keyword

# Chunking (important for quality)
MAX_CHUNK_SIZE = 1000                         # Tokens per chunk
CHUNK_OVERLAP = 50                            # Token overlap

# Embedding model
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# Indexing
AUTO_INDEX_ON_START = False                   # Reindex on startup?
WATCH_DIRECTORY = True                        # Auto-index new files?
```

### Tuning for Your Use Case

**Many short documents (papers, abstracts):**
```python
MAX_CHUNK_SIZE = 500
CHUNK_OVERLAP = 25
DEFAULT_ALPHA = 0.6  # Favor semantics
```

**Few long documents (theses, books):**
```python
MAX_CHUNK_SIZE = 2000
CHUNK_OVERLAP = 100
DEFAULT_ALPHA = 0.4  # Favor keywords
```

**Highly technical content (lots of acronyms):**
```python
DEFAULT_ALPHA = 0.3  # More keyword-focused
```

## ğŸ“Š Performance Characteristics

### Indexing

| Metric | Time | Notes |
|--------|------|-------|
| Per markdown file | ~50-100ms | Depends on size |
| Per PDF (text) | ~100-200ms | Text extraction |
| Per PDF (scanned) | ~1-5s per page | OCR is slow |
| Embedding generation | ~10ms per document | Depends on chunk count |

### Searching

| Method | Latency | Memory |
|--------|---------|--------|
| Semantic search | 50-100ms | ~1-2GB |
| Keyword search | 10-50ms | ~100-500MB |
| Hybrid (both) | 100-150ms | ~1-2GB |
| With reranking | +100-150ms | Same |

**For 100 papers (~500 chunks):**
- Initial indexing: ~2-5 minutes
- Search latency: <200ms
- Memory usage: 3-5GB (Chroma + embeddings)

## ğŸ§ª Usage Examples

### Example 1: Climate Data Search

```python
# Find papers about MODIS and albedo
results, scores = search_engine.hybrid_search(
    query="MODIS satellite albedo measurements",
    top_k=10,
    alpha=0.6
)

for doc, score in zip(results, scores):
    if score > 0.7:  # High confidence
        print(f"âœ“ {doc['title']} ({score:.1%})")
```

### Example 2: Precise Technical Search

```python
# Find specific sensor data
results, scores = search_engine.hybrid_search(
    query="MOD10A1",
    alpha=0.2  # Mostly keyword-based
)

# Further filter by year
from_2020 = [r for r in results if int(r.get('year', 0)) >= 2020]
```

### Example 3: Multi-criteria Query

```python
# Complex query with metadata filtering
results = search_engine.search(
    query="glacier dynamics",
    where_document={
        "$and": [
            {"$contains": "Alaska"},
            {"$regex": "Landsat|Sentinel"}
        ]
    }
)
```

## ğŸš¦ Troubleshooting

### Issue: Low search quality

**Solution:** Adjust alpha parameter
```python
# Too many irrelevant results?
alpha=0.3  # More keyword focus

# Missing semantically related papers?
alpha=0.8  # More semantic focus
```

### Issue: OCR not working

**Cause:** Tesseract not installed

**Solution:**
```bash
# Windows: Download from GitHub
# Linux:
sudo apt-get install tesseract-ocr
# macOS:
brew install tesseract
```

### Issue: Slow search performance

**Cause 1:** Too many documents (~1000+)
- Consider splitting into smaller indexes

**Cause 2:** Large chunk sizes
- Reduce `MAX_CHUNK_SIZE` in config

**Cause 3:** Embedding model too large
- Use `intfloat/multilingual-e5-base` (smaller, slightly slower)

## ğŸ“ˆ Next Steps / Roadmap

- [ ] Support for vector reranking (cross-encoders)
- [ ] Citation graph analysis
- [ ] Document similarity clustering
- [ ] Query expansion with synonyms
- [ ] Performance optimizations (quantization)
- [ ] Support for spreadsheets and tables
- [ ] Web interface for searching

## ğŸ”— Related Resources

- **MCP Protocol**: https://modelcontextprotocol.io/
- **ChromaDB**: https://docs.trychroma.com/
- **Sentence Transformers**: https://sbert.net/
- **FastMCP**: https://github.com/jloops/fastmcp

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“§ Support

For issues, questions, or suggestions:
- Open an issue on GitHub
- Check existing documentation
- Review example scripts in `examples/`

---

**Last Updated:** November 2025
**Current Version:** 0.1.0
**Dependencies Updated:** chromadb 1.3.4, sentence-transformers 5.1.2
